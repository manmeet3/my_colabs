{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMPE 297 Reinforcement Learning\n",
    "### Deep Learning Primer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the convnet example provided on Canvas as a starting point and add the following two features:\n",
    "1. Add He initialization and compare the training results with the base model.\n",
    "2. Add Nadam optimization and compare the training results with the base model.\n",
    "3. Combine the two modification and explain the overall impact of these two enhancements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./minst/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7c84e163764f40995cd882504f6610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./minst/MNIST/raw/train-images-idx3-ubyte.gz to ./minst/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./minst/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a7d7532c6b434d994cdfc542f1dac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./minst/MNIST/raw/train-labels-idx1-ubyte.gz to ./minst/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./minst/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f2c3bd806e47c6803fa2ae078ab991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./minst/MNIST/raw/t10k-images-idx3-ubyte.gz to ./minst/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./minst/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75a84658d5f414cb4bc4eb6f19b39db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./minst/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./minst/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root=\"./minst/\", train=True, # misspelled mnist\n",
    "                                           transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.MNIST(root=\"./minst/\", train=False, \n",
    "                                          transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "          nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "          nn.BatchNorm2d(16),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "          nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "          nn.BatchNorm2d(32),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 2.1873\n",
      "Epoch [1/5], Step [200/600], Loss: 2.3314\n",
      "Epoch [1/5], Step [300/600], Loss: 2.1164\n",
      "Epoch [1/5], Step [400/600], Loss: 2.0874\n",
      "Epoch [1/5], Step [500/600], Loss: 2.0558\n",
      "Epoch [1/5], Step [600/600], Loss: 2.2396\n",
      "Epoch [2/5], Step [100/600], Loss: 2.0795\n",
      "Epoch [2/5], Step [200/600], Loss: 2.0834\n",
      "Epoch [2/5], Step [300/600], Loss: 2.1748\n",
      "Epoch [2/5], Step [400/600], Loss: 1.9754\n",
      "Epoch [2/5], Step [500/600], Loss: 2.0241\n",
      "Epoch [2/5], Step [600/600], Loss: 1.9306\n",
      "Epoch [3/5], Step [100/600], Loss: 2.0278\n",
      "Epoch [3/5], Step [200/600], Loss: 2.1669\n",
      "Epoch [3/5], Step [300/600], Loss: 2.2082\n",
      "Epoch [3/5], Step [400/600], Loss: 1.9605\n",
      "Epoch [3/5], Step [500/600], Loss: 1.9214\n",
      "Epoch [3/5], Step [600/600], Loss: 1.9061\n",
      "Epoch [4/5], Step [100/600], Loss: 1.9747\n",
      "Epoch [4/5], Step [200/600], Loss: 2.0738\n",
      "Epoch [4/5], Step [300/600], Loss: 1.9923\n",
      "Epoch [4/5], Step [400/600], Loss: 1.9831\n",
      "Epoch [4/5], Step [500/600], Loss: 1.9613\n",
      "Epoch [4/5], Step [600/600], Loss: 1.8804\n",
      "Epoch [5/5], Step [100/600], Loss: 1.8303\n",
      "Epoch [5/5], Step [200/600], Loss: 2.0288\n",
      "Epoch [5/5], Step [300/600], Loss: 1.9744\n",
      "Epoch [5/5], Step [400/600], Loss: 1.9174\n",
      "Epoch [5/5], Step [500/600], Loss: 1.9392\n",
      "Epoch [5/5], Step [600/600], Loss: 2.0508\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 31.97 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'default.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## He initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "          nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "          nn.BatchNorm2d(16),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "          nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "          nn.BatchNorm2d(32),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        # Apply HE Initialization\n",
    "        nn.init.kaiming_uniform_(self.layer1[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.layer2[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.fc.weight, mode='fan_out', nonlinearity='relu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 4.6500\n",
      "Epoch [1/5], Step [200/600], Loss: 3.9968\n",
      "Epoch [1/5], Step [300/600], Loss: 3.4714\n",
      "Epoch [1/5], Step [400/600], Loss: 3.1483\n",
      "Epoch [1/5], Step [500/600], Loss: 2.9116\n",
      "Epoch [1/5], Step [600/600], Loss: 2.7317\n",
      "Epoch [2/5], Step [100/600], Loss: 3.0870\n",
      "Epoch [2/5], Step [200/600], Loss: 2.6206\n",
      "Epoch [2/5], Step [300/600], Loss: 2.7175\n",
      "Epoch [2/5], Step [400/600], Loss: 2.5413\n",
      "Epoch [2/5], Step [500/600], Loss: 2.6289\n",
      "Epoch [2/5], Step [600/600], Loss: 2.5128\n",
      "Epoch [3/5], Step [100/600], Loss: 2.6133\n",
      "Epoch [3/5], Step [200/600], Loss: 2.4364\n",
      "Epoch [3/5], Step [300/600], Loss: 2.2037\n",
      "Epoch [3/5], Step [400/600], Loss: 2.3789\n",
      "Epoch [3/5], Step [500/600], Loss: 2.2477\n",
      "Epoch [3/5], Step [600/600], Loss: 2.2576\n",
      "Epoch [4/5], Step [100/600], Loss: 2.2401\n",
      "Epoch [4/5], Step [200/600], Loss: 2.2923\n",
      "Epoch [4/5], Step [300/600], Loss: 2.2653\n",
      "Epoch [4/5], Step [400/600], Loss: 2.3265\n",
      "Epoch [4/5], Step [500/600], Loss: 2.1498\n",
      "Epoch [4/5], Step [600/600], Loss: 2.2253\n",
      "Epoch [5/5], Step [100/600], Loss: 2.0293\n",
      "Epoch [5/5], Step [200/600], Loss: 2.2834\n",
      "Epoch [5/5], Step [300/600], Loss: 2.0571\n",
      "Epoch [5/5], Step [400/600], Loss: 2.2185\n",
      "Epoch [5/5], Step [500/600], Loss: 2.0715\n",
      "Epoch [5/5], Step [600/600], Loss: 2.0808\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 30.34 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'he_init.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Nadam optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neuralnet-pytorch\n",
      "  Downloading neuralnet_pytorch-0.0.3-py3-none-any.whl (29 kB)\n",
      "Collecting visdom\n",
      "  Downloading visdom-0.1.8.9.tar.gz (676 kB)\n",
      "\u001b[K     |████████████████████████████████| 676 kB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from neuralnet-pytorch) (1.18.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from neuralnet-pytorch) (1.4.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from neuralnet-pytorch) (3.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from visdom->neuralnet-pytorch) (2.24.0)\n",
      "Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from visdom->neuralnet-pytorch) (6.0.4)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from visdom->neuralnet-pytorch) (19.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from visdom->neuralnet-pytorch) (1.15.0)\n",
      "Collecting jsonpatch\n",
      "  Downloading jsonpatch-1.26-py2.py3-none-any.whl (11 kB)\n",
      "Collecting torchfile\n",
      "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
      "Collecting websocket-client\n",
      "  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from visdom->neuralnet-pytorch) (7.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib->neuralnet-pytorch) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->neuralnet-pytorch) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->neuralnet-pytorch) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->neuralnet-pytorch) (2.8.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->neuralnet-pytorch) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->neuralnet-pytorch) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->visdom->neuralnet-pytorch) (2.6)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->neuralnet-pytorch) (3.0.4)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-2.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Building wheels for collected packages: visdom, torchfile\n",
      "  Building wheel for visdom (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for visdom: filename=visdom-0.1.8.9-py3-none-any.whl size=668534 sha256=99c5731a9ce541d375a6fd930767b59d28dedb8feb43e3045f148ff18c14bba3\n",
      "  Stored in directory: /root/.cache/pip/wheels/2d/cd/fb/005445070865d4e45365b2946ee88085a7392370f152cf371c\n",
      "  Building wheel for torchfile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=6623 sha256=c2a04a5cec404ff9b2c0ae6c20ac55582713e44745b6e49141aa99dcceb7a9d2\n",
      "  Stored in directory: /root/.cache/pip/wheels/55/79/ec/084a3a2e348d72852cc0c13c559c923c13ca54db86e699b681\n",
      "Successfully built visdom torchfile\n",
      "Installing collected packages: jsonpointer, jsonpatch, torchfile, websocket-client, visdom, neuralnet-pytorch\n",
      "Successfully installed jsonpatch-1.26 jsonpointer-2.0 neuralnet-pytorch-0.0.3 torchfile-0.1.0 visdom-0.1.8.9 websocket-client-0.57.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install neuralnet-pytorch imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuralnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "          nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "          nn.BatchNorm2d(16),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "          nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "          nn.BatchNorm2d(32),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = neuralnet_pytorch.NAdam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/neuralnet_pytorch/optimizer.py:51: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  exp_avg.mul_(beta1).add_(1. - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 5.2880\n",
      "Epoch [1/5], Step [200/600], Loss: 4.4494\n",
      "Epoch [1/5], Step [300/600], Loss: 4.0515\n",
      "Epoch [1/5], Step [400/600], Loss: 3.5833\n",
      "Epoch [1/5], Step [500/600], Loss: 3.6037\n",
      "Epoch [1/5], Step [600/600], Loss: 3.3639\n",
      "Epoch [2/5], Step [100/600], Loss: 3.3315\n",
      "Epoch [2/5], Step [200/600], Loss: 3.2858\n",
      "Epoch [2/5], Step [300/600], Loss: 3.1180\n",
      "Epoch [2/5], Step [400/600], Loss: 2.9006\n",
      "Epoch [2/5], Step [500/600], Loss: 2.7481\n",
      "Epoch [2/5], Step [600/600], Loss: 2.8206\n",
      "Epoch [3/5], Step [100/600], Loss: 2.5571\n",
      "Epoch [3/5], Step [200/600], Loss: 2.6786\n",
      "Epoch [3/5], Step [300/600], Loss: 2.5965\n",
      "Epoch [3/5], Step [400/600], Loss: 2.7189\n",
      "Epoch [3/5], Step [500/600], Loss: 2.5686\n",
      "Epoch [3/5], Step [600/600], Loss: 2.3803\n",
      "Epoch [4/5], Step [100/600], Loss: 2.4954\n",
      "Epoch [4/5], Step [200/600], Loss: 2.5260\n",
      "Epoch [4/5], Step [300/600], Loss: 2.4473\n",
      "Epoch [4/5], Step [400/600], Loss: 2.2835\n",
      "Epoch [4/5], Step [500/600], Loss: 2.3270\n",
      "Epoch [4/5], Step [600/600], Loss: 2.5078\n",
      "Epoch [5/5], Step [100/600], Loss: 2.3419\n",
      "Epoch [5/5], Step [200/600], Loss: 2.1726\n",
      "Epoch [5/5], Step [300/600], Loss: 2.0640\n",
      "Epoch [5/5], Step [400/600], Loss: 2.1575\n",
      "Epoch [5/5], Step [500/600], Loss: 2.2661\n",
      "Epoch [5/5], Step [600/600], Loss: 2.1042\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 28.17 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'nadam.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nadam and HE initialization combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "          nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "          nn.BatchNorm2d(16),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "          nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "          nn.BatchNorm2d(32),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        # Apply HE Initialization\n",
    "        nn.init.kaiming_uniform_(self.layer1[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.layer2[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.fc.weight, mode='fan_out', nonlinearity='relu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = neuralnet_pytorch.NAdam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 5.3969\n",
      "Epoch [1/5], Step [200/600], Loss: 4.6659\n",
      "Epoch [1/5], Step [300/600], Loss: 4.1772\n",
      "Epoch [1/5], Step [400/600], Loss: 4.0554\n",
      "Epoch [1/5], Step [500/600], Loss: 3.8239\n",
      "Epoch [1/5], Step [600/600], Loss: 3.3925\n",
      "Epoch [2/5], Step [100/600], Loss: 3.0167\n",
      "Epoch [2/5], Step [200/600], Loss: 3.2880\n",
      "Epoch [2/5], Step [300/600], Loss: 2.9667\n",
      "Epoch [2/5], Step [400/600], Loss: 3.2483\n",
      "Epoch [2/5], Step [500/600], Loss: 2.9733\n",
      "Epoch [2/5], Step [600/600], Loss: 2.5645\n",
      "Epoch [3/5], Step [100/600], Loss: 2.6773\n",
      "Epoch [3/5], Step [200/600], Loss: 2.6239\n",
      "Epoch [3/5], Step [300/600], Loss: 2.7314\n",
      "Epoch [3/5], Step [400/600], Loss: 2.6112\n",
      "Epoch [3/5], Step [500/600], Loss: 2.6661\n",
      "Epoch [3/5], Step [600/600], Loss: 2.6085\n",
      "Epoch [4/5], Step [100/600], Loss: 2.2669\n",
      "Epoch [4/5], Step [200/600], Loss: 2.3850\n",
      "Epoch [4/5], Step [300/600], Loss: 2.3502\n",
      "Epoch [4/5], Step [400/600], Loss: 2.5026\n",
      "Epoch [4/5], Step [500/600], Loss: 2.3448\n",
      "Epoch [4/5], Step [600/600], Loss: 2.3792\n",
      "Epoch [5/5], Step [100/600], Loss: 2.1139\n",
      "Epoch [5/5], Step [200/600], Loss: 2.2344\n",
      "Epoch [5/5], Step [300/600], Loss: 2.4215\n",
      "Epoch [5/5], Step [400/600], Loss: 2.4398\n",
      "Epoch [5/5], Step [500/600], Loss: 2.3320\n",
      "Epoch [5/5], Step [600/600], Loss: 2.2968\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 27.53 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'he_nadam_combined.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writeup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
