{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMPE 297 Reinforcement Learning\n",
    "### Deep Learning Primer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the convnet example provided on Canvas as a starting point and add the following two features:\n",
    "1. Add He initialization and compare the training results with the base model.\n",
    "2. Add Nadam optimization and compare the training results with the base model.\n",
    "3. Combine the two modification and explain the overall impact of these two enhancements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root=\"./minst/\", train=True, # misspelled mnist\n",
    "                                           transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.MNIST(root=\"./minst/\", train=False, \n",
    "                                          transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "          nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "          nn.BatchNorm2d(16),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "          nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "          nn.BatchNorm2d(32),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.1642\n",
      "Epoch [1/5], Step [200/600], Loss: 0.1952\n",
      "Epoch [1/5], Step [300/600], Loss: 0.0635\n",
      "Epoch [1/5], Step [400/600], Loss: 0.0808\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1633\n",
      "Epoch [1/5], Step [600/600], Loss: 0.0749\n",
      "Epoch [2/5], Step [100/600], Loss: 0.0548\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0367\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0284\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0219\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0054\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0222\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0041\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0605\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0118\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0146\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0274\n",
      "Epoch [3/5], Step [600/600], Loss: 0.1384\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0365\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0090\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0298\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0317\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0057\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0166\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0672\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0280\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0064\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0254\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0037\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0420\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 98.64 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'default.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## He initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "          nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "          nn.BatchNorm2d(16),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "          nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "          nn.BatchNorm2d(32),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        # Apply HE Initialization\n",
    "        nn.init.kaiming_uniform_(self.layer1[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.layer2[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.fc.weight, mode='fan_out', nonlinearity='relu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.8453\n",
      "Epoch [1/5], Step [200/600], Loss: 0.3935\n",
      "Epoch [1/5], Step [300/600], Loss: 0.4081\n",
      "Epoch [1/5], Step [400/600], Loss: 0.3112\n",
      "Epoch [1/5], Step [500/600], Loss: 0.4268\n",
      "Epoch [1/5], Step [600/600], Loss: 0.1883\n",
      "Epoch [2/5], Step [100/600], Loss: 0.1137\n",
      "Epoch [2/5], Step [200/600], Loss: 0.1338\n",
      "Epoch [2/5], Step [300/600], Loss: 0.2112\n",
      "Epoch [2/5], Step [400/600], Loss: 0.1894\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0248\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0872\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0534\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0761\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0813\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0441\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0121\n",
      "Epoch [3/5], Step [600/600], Loss: 0.1362\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0373\n",
      "Epoch [4/5], Step [200/600], Loss: 0.1048\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0045\n",
      "Epoch [4/5], Step [400/600], Loss: 0.1885\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0039\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0536\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0718\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0805\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0670\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0936\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0175\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0327\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 97.69 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'he_init.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Nadam optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imageio\n",
      "  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting neuralnet-pytorch\n",
      "  Downloading neuralnet_pytorch-0.0.3-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (7.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.18.5)\n",
      "Collecting visdom\n",
      "  Downloading visdom-0.1.8.9.tar.gz (676 kB)\n",
      "\u001b[K     |████████████████████████████████| 676 kB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from neuralnet-pytorch) (3.3.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from neuralnet-pytorch) (1.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from visdom->neuralnet-pytorch) (2.24.0)\n",
      "Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from visdom->neuralnet-pytorch) (6.0.4)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from visdom->neuralnet-pytorch) (19.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from visdom->neuralnet-pytorch) (1.15.0)\n",
      "Collecting jsonpatch\n",
      "  Downloading jsonpatch-1.26-py2.py3-none-any.whl (11 kB)\n",
      "Collecting torchfile\n",
      "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
      "Collecting websocket-client\n",
      "  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->neuralnet-pytorch) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->neuralnet-pytorch) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib->neuralnet-pytorch) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->neuralnet-pytorch) (0.10.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->neuralnet-pytorch) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->visdom->neuralnet-pytorch) (2.6)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->neuralnet-pytorch) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->neuralnet-pytorch) (2020.6.20)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-2.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Building wheels for collected packages: visdom, torchfile\n",
      "  Building wheel for visdom (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for visdom: filename=visdom-0.1.8.9-py3-none-any.whl size=668534 sha256=654f06ef1c49385f5780d4a6bccd05b1e7f66169620a0d1702bdb3516790c76b\n",
      "  Stored in directory: /root/.cache/pip/wheels/2d/cd/fb/005445070865d4e45365b2946ee88085a7392370f152cf371c\n",
      "  Building wheel for torchfile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=6623 sha256=b920f6d7383a502fa8bc0fb761003d92443f398d2504ea626eb3fd26d98be5e5\n",
      "  Stored in directory: /root/.cache/pip/wheels/55/79/ec/084a3a2e348d72852cc0c13c559c923c13ca54db86e699b681\n",
      "Successfully built visdom torchfile\n",
      "Installing collected packages: imageio, jsonpointer, jsonpatch, torchfile, websocket-client, visdom, neuralnet-pytorch\n",
      "Successfully installed imageio-2.9.0 jsonpatch-1.26 jsonpointer-2.0 neuralnet-pytorch-0.0.3 torchfile-0.1.0 visdom-0.1.8.9 websocket-client-0.57.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install imageio neuralnet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuralnet_pytorch\n",
    "# Ref: https://neuralnet-pytorch.readthedocs.io/en/latest/manual/optimization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "          nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "          nn.BatchNorm2d(16),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "          nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "          nn.BatchNorm2d(32),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = neuralnet_pytorch.NAdam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/neuralnet_pytorch/optimizer.py:51: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  exp_avg.mul_(beta1).add_(1. - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.2135\n",
      "Epoch [1/5], Step [200/600], Loss: 0.1246\n",
      "Epoch [1/5], Step [300/600], Loss: 0.0648\n",
      "Epoch [1/5], Step [400/600], Loss: 0.1498\n",
      "Epoch [1/5], Step [500/600], Loss: 0.0406\n",
      "Epoch [1/5], Step [600/600], Loss: 0.0714\n",
      "Epoch [2/5], Step [100/600], Loss: 0.0347\n",
      "Epoch [2/5], Step [200/600], Loss: 0.1074\n",
      "Epoch [2/5], Step [300/600], Loss: 0.1216\n",
      "Epoch [2/5], Step [400/600], Loss: 0.1042\n",
      "Epoch [2/5], Step [500/600], Loss: 0.1053\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0048\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0132\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0411\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0242\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0403\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0912\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0205\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0586\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0093\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0256\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0199\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0215\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0124\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0086\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0202\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0356\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0151\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0247\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0075\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 98.62 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'nadam.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nadam and HE initialization combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "          nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "          nn.BatchNorm2d(16),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "          nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "          nn.BatchNorm2d(32),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        # Apply HE Initialization\n",
    "        nn.init.kaiming_uniform_(self.layer1[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.layer2[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.fc.weight, mode='fan_out', nonlinearity='relu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = neuralnet_pytorch.NAdam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 1.4416\n",
      "Epoch [1/5], Step [200/600], Loss: 0.5379\n",
      "Epoch [1/5], Step [300/600], Loss: 0.4891\n",
      "Epoch [1/5], Step [400/600], Loss: 0.1039\n",
      "Epoch [1/5], Step [500/600], Loss: 0.4146\n",
      "Epoch [1/5], Step [600/600], Loss: 0.2820\n",
      "Epoch [2/5], Step [100/600], Loss: 0.2315\n",
      "Epoch [2/5], Step [200/600], Loss: 0.1208\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0760\n",
      "Epoch [2/5], Step [400/600], Loss: 0.1192\n",
      "Epoch [2/5], Step [500/600], Loss: 0.1658\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0931\n",
      "Epoch [3/5], Step [100/600], Loss: 0.1842\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0097\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0518\n",
      "Epoch [3/5], Step [400/600], Loss: 0.1655\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0483\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0426\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0600\n",
      "Epoch [4/5], Step [200/600], Loss: 0.1235\n",
      "Epoch [4/5], Step [300/600], Loss: 0.1638\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0333\n",
      "Epoch [4/5], Step [500/600], Loss: 0.1088\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0869\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0082\n",
      "Epoch [5/5], Step [200/600], Loss: 0.1016\n",
      "Epoch [5/5], Step [300/600], Loss: 0.1314\n",
      "Epoch [5/5], Step [400/600], Loss: 0.1127\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0517\n",
      "Epoch [5/5], Step [600/600], Loss: 0.1037\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 97.28 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'he_nadam_combined.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Configuration           | Accuracy |\n",
    "|-------------------|-------|\n",
    "| Default           | 98.64 |\n",
    "| He initialization | 97.69 |\n",
    "| Nadam optim       | 98.62 |\n",
    "| He + Nadam        | 97.28 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
